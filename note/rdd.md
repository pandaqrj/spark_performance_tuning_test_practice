# RDD执行原理：
Spark在执行时先申请资源，然后将应用程序的数据处理逻辑分解成一个一个的计算任务。  
然后将任务发送到已经分配资源的计算节点上，按照制定的计算模型进行数据计算，最后得到结果  
RDD是Spark用于数据处理的核心模型
## 在YARN环境中，RDD的工作步骤和原理：
1. 启动YARN集群环境
2. Spark申请资源创建Driver、Executor
3. Spark根据需求将计算逻辑分局分区划分成不同的任务，有Driver执行，生产TaskPool
4. Driver将任务根据计算节点的状态（数据或者其他）发动到对应的节点的Executor进行计算  

因此RDD在整个流程中主要用于将逻辑进行封装，并生成Task发动给Executor执行计算。

